{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105b615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operations import oversampling_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35eeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling = [\"none\", \"regular\", \"sqrt\", \"beta\"]\n",
    "class_gaps = [1, 3, 10, 30, 100]\n",
    "model_parameters = {'embed_size': 128, 'hidden_dim': 235, 'feed_forward_dim': 373, 'num_layers': 3, 'num_heads': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189d31f",
   "metadata": {},
   "source": [
    "# Experiment Overview\n",
    "\n",
    "I conducted a supplementary experiment employing the Transformer architecture to investigate the effects of various oversampling strategies on addressing the challenges posed by imbalanced class distributions in the PFAM dataset. My objective was to examine the impact of these oversampling approaches on the F1 score, a critical performance metric in protein family classification, particularly in the presence of imbalanced datasets.\n",
    "\n",
    "To create the five subsets of the PFAM dataset with distinct levels of imbalance, I selected 100 classes of varying popularity for each train|val|test split. Popularity, in this context, is a measure of the number of samples per class in the dataset. The classes were chosen starting with the most popular and incrementing through the dataset with factors of 1, 3, 10, 30, and 100, resulting in subsets designated as Iter1, Iter3, Iter10, Iter30, and Iter100.\n",
    "\n",
    "For each dataset, I assessed four distinct oversampling methods: \"none\", \"regular\", \"sqrt\", and \"efficient\".\n",
    "\n",
    "None: In this baseline approach, no oversampling was applied during the training and validation steps, enabling me to assess the performance of the Transformer architecture without any sampling modifications.\n",
    "\n",
    "Regular: Building upon the baseline, I implemented a straightforward oversampling method that adjusts sample selection probabilities inversely proportional to class frequencies. This approach enhances the representation of underrepresented classes, potentially improving overall classification performance.\n",
    "\n",
    "Sqrt: To further refine the oversampling process, I introduced a square root weighting scheme that adjusts the class probabilities, increasing the likelihood of sampling less common classes while mitigating the risk of overfitting associated with the regular oversampling method.\n",
    "\n",
    "Efficient: In my final approach, I employed a re-weighting method based on the concept of \"effective number of samples\", as proposed in a CVPR'19 research paper by Google. This method introduces a parameter, ùõΩ, and aims to strike a balance between regular oversampling and more complex techniques, offering the potential for superior classification performance on imbalanced datasets.\n",
    "\n",
    "The performance of each oversampling method was evaluated using the F1 score on a test set, with models being trained using separate training and validation sets. All testing was conducted using \"regular\" oversampling for a fair comparison of the learning capabilities of less visible classes across different techniques.\n",
    "\n",
    "Through this systematic investigation, I aimed to provide valuable insights into the effectiveness of various oversampling approaches in addressing imbalanced class distributions in the context of protein family classification using Transformer models. The results of my study may have practical implications for researchers and practitioners in the field of bioinformatics and drug discovery, enabling them to more effectively classify protein sequences and identify potential drug targets for developing new treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8428f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gap 1 with oversampling none.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.04, train f1: 0.29, val loss: 0.03, val f1: 0.55, duration: 32.8s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.94, val loss: 0.00, val f1: 0.94, duration: 36.0s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.98, duration: 35.7s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9850182158396913\n",
      "\n",
      "\n",
      "\n",
      "Training gap 3 with oversampling none.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.24, val loss: 0.03, val f1: 0.48, duration: 27.5s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.93, val loss: 0.00, val f1: 0.94, duration: 28.8s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.97, val loss: 0.00, val f1: 0.98, duration: 28.7s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9766876674319\n",
      "\n",
      "\n",
      "\n",
      "Training gap 10 with oversampling none.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.10, val loss: 0.04, val f1: 0.17, duration: 19.4s\n",
      "Epoch   5/ 10, train loss: 0.01, train f1: 0.82, val loss: 0.01, val f1: 0.83, duration: 18.6s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.93, val loss: 0.00, val f1: 0.93, duration: 18.5s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9268734823353347\n",
      "\n",
      "\n",
      "\n",
      "Training gap 30 with oversampling none.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.04, val loss: 0.06, val f1: 0.07, duration: 10.4s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.48, val loss: 0.04, val f1: 0.33, duration: 10.2s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.70, val loss: 0.02, val f1: 0.60, duration: 10.1s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.6280952523026735\n",
      "\n",
      "\n",
      "\n",
      "Training gap 100 with oversampling none.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.02, val loss: 0.07, val f1: 0.01, duration: 5.2s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.23, val loss: 0.05, val f1: 0.17, duration: 5.1s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.57, val loss: 0.03, val f1: 0.45, duration: 5.3s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.44042040329873416\n",
      "\n",
      "\n",
      "\n",
      "Training gap 1 with oversampling regular.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.04, train f1: 0.33, val loss: 0.03, val f1: 0.64, duration: 36.2s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.94, val loss: 0.00, val f1: 0.95, duration: 36.0s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.99, duration: 36.2s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9872607920373347\n",
      "\n",
      "\n",
      "\n",
      "Training gap 3 with oversampling regular.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.29, val loss: 0.03, val f1: 0.54, duration: 27.9s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.93, val loss: 0.00, val f1: 0.95, duration: 28.2s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.98, duration: 28.5s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9835771966448792\n",
      "\n",
      "\n",
      "\n",
      "Training gap 10 with oversampling regular.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.19, val loss: 0.04, val f1: 0.32, duration: 17.8s\n",
      "Epoch   5/ 10, train loss: 0.01, train f1: 0.85, val loss: 0.01, val f1: 0.87, duration: 18.5s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.94, val loss: 0.00, val f1: 0.95, duration: 19.1s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9466478467502926\n",
      "\n",
      "\n",
      "\n",
      "Training gap 30 with oversampling regular.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.06, train f1: 0.11, val loss: 0.05, val f1: 0.19, duration: 11.0s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.62, val loss: 0.03, val f1: 0.49, duration: 10.7s\n",
      "Epoch  10/ 10, train loss: 0.02, train f1: 0.76, val loss: 0.01, val f1: 0.75, duration: 10.5s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.7430678088022498\n",
      "\n",
      "\n",
      "\n",
      "Training gap 100 with oversampling regular.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.06, train f1: 0.06, val loss: 0.06, val f1: 0.08, duration: 5.1s\n",
      "Epoch   5/ 10, train loss: 0.03, train f1: 0.56, val loss: 0.03, val f1: 0.57, duration: 5.6s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.82, val loss: 0.01, val f1: 0.80, duration: 5.4s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.82177727411851\n",
      "\n",
      "\n",
      "\n",
      "Training gap 1 with oversampling sqrt.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.04, train f1: 0.33, val loss: 0.03, val f1: 0.59, duration: 36.1s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.95, val loss: 0.00, val f1: 0.96, duration: 37.7s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.98, duration: 35.4s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9852584094949841\n",
      "\n",
      "\n",
      "\n",
      "Training gap 3 with oversampling sqrt.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.28, val loss: 0.03, val f1: 0.48, duration: 27.4s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.93, val loss: 0.00, val f1: 0.94, duration: 28.7s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.98, duration: 30.9s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.981151458804347\n",
      "\n",
      "\n",
      "\n",
      "Training gap 10 with oversampling sqrt.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.14, val loss: 0.04, val f1: 0.27, duration: 19.7s\n",
      "Epoch   5/ 10, train loss: 0.01, train f1: 0.84, val loss: 0.01, val f1: 0.87, duration: 17.8s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.94, val loss: 0.00, val f1: 0.95, duration: 18.1s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9472495666754491\n",
      "\n",
      "\n",
      "\n",
      "Training gap 30 with oversampling sqrt.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.06, train f1: 0.06, val loss: 0.05, val f1: 0.09, duration: 11.0s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.58, val loss: 0.03, val f1: 0.53, duration: 10.5s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.78, val loss: 0.02, val f1: 0.72, duration: 10.5s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.7164578083427428\n",
      "\n",
      "\n",
      "\n",
      "Training gap 100 with oversampling sqrt.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.06, train f1: 0.03, val loss: 0.06, val f1: 0.07, duration: 5.2s\n",
      "Epoch   5/ 10, train loss: 0.03, train f1: 0.44, val loss: 0.03, val f1: 0.46, duration: 5.4s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.77, val loss: 0.02, val f1: 0.76, duration: 5.0s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.7644219893133976\n",
      "\n",
      "\n",
      "\n",
      "Training gap 1 with oversampling beta.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.04, train f1: 0.29, val loss: 0.03, val f1: 0.55, duration: 36.2s\n",
      "Epoch   5/ 10, train loss: 0.00, train f1: 0.95, val loss: 0.00, val f1: 0.96, duration: 36.7s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.98, val loss: 0.00, val f1: 0.98, duration: 36.3s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9841236502105604\n",
      "\n",
      "\n",
      "\n",
      "Training gap 3 with oversampling beta.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.23, val loss: 0.03, val f1: 0.45, duration: 27.9s\n",
      "Epoch   5/ 10, train loss: 0.01, train f1: 0.90, val loss: 0.00, val f1: 0.93, duration: 27.8s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.97, val loss: 0.00, val f1: 0.98, duration: 28.1s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.9768875267220756\n",
      "\n",
      "\n",
      "\n",
      "Training gap 10 with oversampling beta.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.10, val loss: 0.04, val f1: 0.17, duration: 17.8s\n",
      "Epoch   5/ 10, train loss: 0.01, train f1: 0.82, val loss: 0.01, val f1: 0.84, duration: 18.6s\n",
      "Epoch  10/ 10, train loss: 0.00, train f1: 0.93, val loss: 0.00, val f1: 0.94, duration: 18.2s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.933422173027994\n",
      "\n",
      "\n",
      "\n",
      "Training gap 30 with oversampling beta.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n",
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.04, val loss: 0.06, val f1: 0.04, duration: 9.9s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.47, val loss: 0.03, val f1: 0.42, duration: 10.2s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.71, val loss: 0.02, val f1: 0.66, duration: 10.1s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.6378060650159864\n",
      "\n",
      "\n",
      "\n",
      "Training gap 100 with oversampling beta.\n",
      "Transformer Model Best Params  | hd 235, nl 3, ne 128, ff 373, nh 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/ 10, train loss: 0.05, train f1: 0.02, val loss: 0.07, val f1: 0.00, duration: 5.2s\n",
      "Epoch   5/ 10, train loss: 0.02, train f1: 0.24, val loss: 0.05, val f1: 0.18, duration: 5.2s\n",
      "Epoch  10/ 10, train loss: 0.01, train f1: 0.56, val loss: 0.03, val f1: 0.49, duration: 5.3s\n",
      "\n",
      "\n",
      "\n",
      "Test F1 score: 0.4361624423919399\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampling_training(oversampling, class_gaps, model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73019027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
