{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2e62e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences as pad\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from Models import FullyConnected, RNN, RNNEmbed, LSTM, BiLSTM, Transformer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics.functional.classification import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "766c4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PfamDataset(Dataset):\n",
    "    \n",
    "    # Initiating the label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    def __init__(self, title, num_classes, k, mapping, max_seq_len, oversampling):\n",
    "        self.title = title\n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.oversampling = oversampling\n",
    "        self.mapping = mapping\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.weights = None\n",
    "        self.len = None\n",
    "        self.__initiate__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "    \n",
    "    def __initiate__(self):\n",
    "        ''' Loads the datasets and generate X and Y'''\n",
    "        path_post = \"\" if self.k == \"all\" else \"_\" + str(self.k)\n",
    "        self.data = pd.read_csv(\"data/\" + self.title + path_post + \".csv\")\n",
    "        self.X = self.get_inputs()\n",
    "        self.y = self.get_labels()\n",
    "        self.weights = self.get_class_weights()\n",
    "        \n",
    "    def replace_uncommon_amino_acids(self):\n",
    "        ''' Replaces uncommon amino acids in sequences with a collective \"X\" '''\n",
    "        replace_minority_aa = lambda data: [re.sub(r\"[XUBOZ]\", \"X\", sequence) \\\n",
    "                                            for sequence in data[\"sequence\"]]\n",
    "        self.data[\"sequence\"] = replace_minority_aa(self.data)\n",
    "        \n",
    "    def get_inputs(self, padding=\"pre\"):\n",
    "        ''' Returns the sequences in a format suitable for our model '''\n",
    "        self.replace_uncommon_amino_acids()\n",
    "        encode = lambda sequence: [self.mapping[aa] for aa in sequence[:self.max_seq_len - 2]]\n",
    "        sequence_list = [encode(sequence) for sequence in self.data[\"sequence\"]]\n",
    "        sequence_list = [[22] + sequence + [23] for sequence in sequence_list]\n",
    "        return torch.tensor(pad(sequence_list, maxlen=self.max_seq_len, \n",
    "                                padding=padding, truncating=\"post\")).float()\n",
    "        \n",
    "    def get_labels(self):\n",
    "        ''' Return the labels in a format suitable for the model'''\n",
    "        if self.title == \"train\" or self.title == \"test\":\n",
    "            encoded_labels = self.label_encoder.fit_transform(self.data[\"family_accession\"])\n",
    "        else:\n",
    "            encoded_labels = self.label_encoder.transform(self.data[\"family_accession\"])\n",
    "        return F.one_hot(torch.tensor(encoded_labels).long(), \n",
    "                         num_classes=self.num_classes).float()\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        ''' Returns the calss weights for use in WeightedRandomSampler'''\n",
    "        class_counts = self.data[\"family_accession\"].value_counts()\n",
    "        class_counts_dict = (1 / class_counts).to_dict()\n",
    "        if self.oversampling == \"regular\":\n",
    "            self.data[\"family_weight\"] = [class_counts_dict[x] for x in self.data[\"family_accession\"]]\n",
    "        elif self.oversampling == \"sqrt\":\n",
    "            self.data[\"family_weight\"] = [np.sqrt(class_counts_dict[x]) for x in self.data[\"family_accession\"]]\n",
    "        elif self.oversampling == \"beta\":\n",
    "            beta = 0.9\n",
    "            class_counts_dict = class_counts.to_dict()\n",
    "            class_counts_dict = {i: 1 / ((1 - beta**c)/(1 - beta)) for (i, c) in class_counts_dict.items()}\n",
    "            self.data[\"family_weight\"] = [class_counts_dict[x] for x in self.data[\"family_accession\"]]\n",
    "        else:\n",
    "            self.data[\"family_weight\"] = [1] * len(self.data[\"family_accession\"])\n",
    "        self.len = len(self.data[\"family_weight\"])\n",
    "        return self.data[\"family_weight\"].to_list()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05c16d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(\n",
    "    ''' The main loop for training the models '''\n",
    "    label: str,\n",
    "    model_parameters: dict,\n",
    "    oversampling: str = \"none\",\n",
    "    epochs: int = 10, \n",
    "    num_classes: int = 100,\n",
    "    class_gap: int = 1,\n",
    "    max_seq_len: int = 128,\n",
    "    vocab_len = 24,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Tensorboard writer\n",
    "    writer = SummaryWriter(log_dir=\"runs/\" + label)\n",
    "                                    # Model _ Hidden Dim _ Layers _ Dropout\n",
    "    \n",
    "    # Mapping\n",
    "    amino_acids = [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\n",
    "                   \"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"Y\",\"X\"]\n",
    "    mapping = {aa:i + 1 for i, aa in enumerate(amino_acids)}\n",
    "    mapping.update({'X': 21, 'U': 21, 'B': 21, 'O': 21, 'Z': 21})\n",
    "    \n",
    "    # Creating the datasets\n",
    "    train_dataset = PfamDataset(\"train\", num_classes, class_gap, \n",
    "                                mapping, max_seq_len, oversampling)\n",
    "    validation_dataset = PfamDataset(\"validation\", num_classes, class_gap, \n",
    "                                     mapping, max_seq_len, oversampling=\"regular\")\n",
    "    \n",
    "    # Weighted Random Sampler\n",
    "    train_sampler = WeightedRandomSampler(train_dataset.weights, train_dataset.len)\n",
    "    validation_sampler = WeightedRandomSampler(validation_dataset.weights, validation_dataset.len)\n",
    "    \n",
    "    # Creating the dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                   num_workers=0, sampler=train_sampler)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                        batch_size=batch_size, \n",
    "                                                        num_workers=0, sampler=validation_sampler)\n",
    "    \n",
    "    # Model\n",
    "    if model_parameters[\"type\"] == \"fc\":\n",
    "        model = FullyConnected(\n",
    "            dropout = model_parameters[\"dropout\"],\n",
    "            hidden_dim = model_parameters[\"hidden_dim\"], \n",
    "            num_layers = model_parameters[\"layers\"],\n",
    "            input_dim = max_seq_len, \n",
    "            output_dim = num_classes, \n",
    "        )\n",
    "    elif model_parameters[\"type\"] == \"rnn\":\n",
    "        model = RNN(\n",
    "            dropout = model_parameters[\"dropout\"],\n",
    "            hidden_dim = model_parameters[\"hidden_dim\"],\n",
    "            num_layers = model_parameters[\"num_layers\"],\n",
    "            vocab_len = vocab_len,\n",
    "            output_dim = num_classes, \n",
    "            device = device, \n",
    "        )\n",
    "    elif model_parameters[\"type\"] == \"rnn_embed\":\n",
    "        model = RNNEmbed(\n",
    "            dropout = model_parameters[\"dropout\"],\n",
    "            hidden_dim = model_parameters[\"hidden_dim\"], \n",
    "            embed_size = model_parameters[\"embed_size\"],\n",
    "            num_layers = model_parameters[\"num_layers\"],\n",
    "            vocab_len = vocab_len,\n",
    "            output_dim = num_classes, \n",
    "            device = device, \n",
    "        )\n",
    "    elif model_parameters[\"type\"] == \"lstm\":\n",
    "        model = LSTM(\n",
    "            dropout = model_parameters[\"dropout\"],\n",
    "            hidden_dim = model_parameters[\"hidden_dim\"], \n",
    "            embed_size = model_parameters[\"embed_size\"],\n",
    "            num_layers = model_parameters[\"num_layers\"],\n",
    "            vocab_len = vocab_len,\n",
    "            output_dim = num_classes, \n",
    "            device = device, \n",
    "        )\n",
    "    elif model_parameters[\"type\"] == \"bi-lstm\":\n",
    "        model = BiLSTM(\n",
    "            dropout = model_parameters[\"dropout\"],\n",
    "            hidden_dim = model_parameters[\"hidden_dim\"], \n",
    "            embed_size = model_parameters[\"embed_size\"],\n",
    "            num_layers = model_parameters[\"num_layers\"],\n",
    "            vocab_len = vocab_len,\n",
    "            output_dim = num_classes, \n",
    "            device = device, \n",
    "        )\n",
    "    elif model_parameters[\"type\"] == \"transformer\":\n",
    "        model = Transformer(\n",
    "            dropout_pos = model_parameters[\"dropout_pos\"],\n",
    "            dropout_transformer = model_parameters[\"dropout_transformer\"],\n",
    "            dropout_class = model_parameters[\"dropout_class\"],\n",
    "            feed_forward_dim = model_parameters[\"feed_forward_dim\"], \n",
    "            hidden_dim = model_parameters[\"hidden_dim\"], \n",
    "            embed_size = model_parameters[\"embed_size\"],\n",
    "            num_layers = model_parameters[\"num_layers\"],\n",
    "            num_heads = model_parameters[\"num_heads\"],\n",
    "            vocab_len = vocab_len,\n",
    "            output_dim = num_classes,\n",
    "        )\n",
    "    model = model.to(device)\n",
    "        \n",
    "    '''\n",
    "    print(\"Model Design (TF Format)\")\n",
    "    summary(model, input_size=(1, max_seq_len))\n",
    "    print(\"\\n\\n\")\n",
    "    '''\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_model_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # --- TRAIN AND EVALUATE ON TRAINING SET \n",
    "        start = time()\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        num_train_correct = 0\n",
    "        num_train_examples = 0\n",
    "\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = loss_function(y_pred, y)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.item() \n",
    "            num_train_correct += (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().item()\n",
    "            num_train_examples += X.shape[0]\n",
    "            \n",
    "        train_acc = num_train_correct / num_train_examples\n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "\n",
    "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        num_val_correct = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = model(X)\n",
    "                loss = loss_function(y_pred, y)\n",
    "\n",
    "                val_loss += loss.data.item()\n",
    "                num_val_correct += (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().item()\n",
    "                num_val_examples += X.shape[0]\n",
    "\n",
    "            val_acc = num_val_correct / num_val_examples\n",
    "            val_loss = val_loss / len(validation_dataloader.dataset)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "          print('Epoch %3d/%3d, train loss: %3.2f, train acc: %3.2f, val loss: %3.2f, val acc: %3.2f, duration: %3.1fs'% \\\n",
    "                (epoch, epochs, train_loss, train_acc, val_loss, val_acc, time() - start))\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Loss/Eval\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Eval\", val_acc, epoch)\n",
    "        \n",
    "        if val_acc > best_model_val_acc:\n",
    "            best_model = deepcopy(model)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    torch.save(best_model, \"model/\" + label + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cd6119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    ''' Evaluates the models against the test set returning the loss and accuracy '''\n",
    "    label: str,\n",
    "    oversampling: str = \"none\",\n",
    "    num_classes: int = 100,\n",
    "    class_gap: int = 1,\n",
    "    max_seq_len: int = 128,\n",
    "    vocab_len = 24,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    amino_acids = [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\n",
    "                   \"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"Y\",\"X\"]\n",
    "    mapping = {aa:i + 1 for i, aa in enumerate(amino_acids)}\n",
    "    mapping.update({'X': 21, 'U': 21, 'B': 21, 'O': 21, 'Z': 21})\n",
    "    \n",
    "    # Creating the datasets\n",
    "    test_dataset = PfamDataset(\"test\", num_classes, class_gap, \n",
    "                               mapping, max_seq_len, oversampling)\n",
    "    \n",
    "    # Weighted Random Sampler\n",
    "    sampler = WeightedRandomSampler(test_dataset.weights, test_dataset.len)\n",
    "    \n",
    "    # Creating the dataloaders\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size, \n",
    "                                                  num_workers=0, sampler=sampler)\n",
    "    \n",
    "    # Model\n",
    "    model = torch.load(\"model/\" + label + \".pt\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    num_val_correct = 0\n",
    "    num_val_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            val_loss += loss.data.item()\n",
    "            num_val_correct += (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().item()\n",
    "            num_val_examples += X.shape[0]\n",
    "\n",
    "        val_acc = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(test_dataloader.dataset)\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790dc3b9",
   "metadata": {},
   "source": [
    "# Architecture Experiment Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af84ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Transformer on Iter1\n",
    "train_evaluate(\n",
    "    label = \"transformer_100\",\n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"transformer\",\n",
    "        \"vocab_len\": 24, \n",
    "        \"embed_size\": 256,\n",
    "        \"hidden_dim\": 128, \n",
    "        \"feed_forward_dim\": 256,\n",
    "        \"dropout_pos\": 0.2, \n",
    "        \"dropout_transformer\": 0.2,\n",
    "        \"dropout_class\": 0.2, \n",
    "        \"num_heads\": 4, \n",
    "        \"num_layers\": 4,\n",
    "    },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Training the Bi-LSTM on Iter1\n",
    "train_evaluate(\n",
    "    label = \"bi-lstm_100\", \n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"bi-lstm\",\n",
    "        \"hidden_dim\": 128, \n",
    "        \"dropout\": 0.2,\n",
    "        \"num_layers\": 4,\n",
    "        \"embed_size\": 64,\n",
    "        },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Training the LSTM on Iter1\n",
    "train_evaluate(\n",
    "    label = \"lstm_100\", \n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"lstm\",\n",
    "        \"hidden_dim\": 256, \n",
    "        \"dropout\": 0.2,\n",
    "        \"num_layers\": 4,\n",
    "        \"embed_size\": 64,\n",
    "        },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Training the RNN on Iter1\n",
    "train_evaluate(\n",
    "    label = \"rnn_embed_100\", \n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"rnn_embed\",\n",
    "        \"hidden_dim\": 256, \n",
    "        \"dropout\": 0.2,\n",
    "        \"num_layers\": 4,\n",
    "        \"embed_size\": 64,\n",
    "        },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Training the RNN (No Embed) on Iter1\n",
    "train_evaluate(\n",
    "    label = \"rnn_100\", \n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"rnn\",\n",
    "        \"hidden_dim\": 256, \n",
    "        \"dropout\": 0.2,\n",
    "        \"num_layers\": 4,\n",
    "        \"embed_size\": 64,\n",
    "        },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Training the FC on Iter1\n",
    "train_evaluate(\n",
    "    label = \"fc_100\", \n",
    "    num_classes = 100,\n",
    "    model_parameters = {\n",
    "        \"type\": \"fc\",\n",
    "        \"dropout\": 0.1,\n",
    "        \"hidden_dim\": 128, \n",
    "        \"layers\": 1,\n",
    "        },\n",
    "    epochs=10\n",
    ")\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d3eae",
   "metadata": {},
   "source": [
    "# Architecture Experiment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4f3b2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(models):\n",
    "    ''' Returns the test loss and accuracy for the model dictionary input '''\n",
    "    test_loss = dict()\n",
    "    test_acc = dict()\n",
    "    for name, label in models.items():\n",
    "        loss, acc = evaluate(label)\n",
    "        test_loss[name] = loss\n",
    "        test_acc[name] = acc\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "66cf92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"fc\": \"fc_100\",\n",
    "    \"rnn\": \"rnn_100\",\n",
    "    \"rnn_embed\": \"rnn_embed_100\",\n",
    "    \"lstm\": \"lstm_100\",\n",
    "    \"bi-lstm\": \"bi-lstm_100\",\n",
    "    \"transformer\": \"transformer_100\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a85b6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_testing(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f8cb7121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:              fc has a loss of        0.03796\n",
      "Architecture:             rnn has a loss of        0.01714\n",
      "Architecture:       rnn_embed has a loss of        0.01072\n",
      "Architecture:            lstm has a loss of        0.00093\n",
      "Architecture:         bi-lstm has a loss of        0.00071\n",
      "Architecture:     transformer has a loss of        0.00103\n"
     ]
    }
   ],
   "source": [
    "for name, result in test_loss.items():\n",
    "    print(\"Architecture: %15s has a loss of %14.5f\" % \\\n",
    "      (name, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ec544953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:              fc has an accuracy of   50.63\n",
      "Architecture:             rnn has an accuracy of   75.37\n",
      "Architecture:       rnn_embed has an accuracy of   83.31\n",
      "Architecture:            lstm has an accuracy of   98.28\n",
      "Architecture:         bi-lstm has an accuracy of   98.75\n",
      "Architecture:     transformer has an accuracy of   98.24\n"
     ]
    }
   ],
   "source": [
    "for name, result in test_acc.items():\n",
    "    result = result * 100\n",
    "    print(\"Architecture: %15s has an accuracy of %7.2f\" % \\\n",
    "      (name, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7500b7",
   "metadata": {},
   "source": [
    "# Oversampling Experiment Training\n",
    "\n",
    "This experiment is explained in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f08b03cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gap 1 with oversampling none.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.24, val loss: 0.04, val acc: 0.41, duration: 16.8s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.84, val loss: 0.01, val acc: 0.86, duration: 16.3s\n",
      "Epoch  10/ 20, train loss: 0.00, train acc: 0.92, val loss: 0.00, val acc: 0.94, duration: 18.6s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.95, val loss: 0.00, val acc: 0.97, duration: 17.9s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.97, val loss: 0.00, val acc: 0.98, duration: 17.1s\n",
      "Training gap 3 with oversampling none.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.21, val loss: 0.04, val acc: 0.31, duration: 12.8s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.81, val loss: 0.01, val acc: 0.82, duration: 13.2s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.92, duration: 13.1s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.94, val loss: 0.00, val acc: 0.96, duration: 13.1s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.96, val loss: 0.00, val acc: 0.97, duration: 13.0s\n",
      "Training gap 10 with oversampling none.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.17, val loss: 0.05, val acc: 0.15, duration: 8.5s\n",
      "Epoch   5/ 20, train loss: 0.02, train acc: 0.73, val loss: 0.02, val acc: 0.70, duration: 8.5s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.85, val loss: 0.01, val acc: 0.82, duration: 8.2s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.90, val loss: 0.01, val acc: 0.88, duration: 8.5s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.93, val loss: 0.00, val acc: 0.92, duration: 8.5s\n",
      "Training gap 30 with oversampling none.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.17, val loss: 0.06, val acc: 0.06, duration: 4.8s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.54, val loss: 0.04, val acc: 0.30, duration: 4.8s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.72, val loss: 0.03, val acc: 0.41, duration: 5.1s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.78, val loss: 0.02, val acc: 0.65, duration: 4.7s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.81, val loss: 0.01, val acc: 0.72, duration: 5.7s\n",
      "Training gap 100 with oversampling none.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.30, val loss: 0.07, val acc: 0.03, duration: 2.9s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.54, val loss: 0.06, val acc: 0.15, duration: 2.7s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.71, val loss: 0.04, val acc: 0.33, duration: 2.4s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.81, val loss: 0.03, val acc: 0.49, duration: 2.4s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.86, val loss: 0.02, val acc: 0.59, duration: 2.4s\n",
      "Training gap 1 with oversampling regular.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.27, val loss: 0.05, val acc: 0.25, duration: 16.9s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.82, val loss: 0.01, val acc: 0.84, duration: 17.0s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.93, duration: 17.0s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.95, val loss: 0.00, val acc: 0.96, duration: 17.0s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.97, val loss: 0.00, val acc: 0.98, duration: 16.9s\n",
      "Training gap 3 with oversampling regular.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.22, val loss: 0.04, val acc: 0.32, duration: 13.2s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.82, val loss: 0.01, val acc: 0.80, duration: 12.9s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.90, val loss: 0.00, val acc: 0.91, duration: 13.2s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.94, val loss: 0.00, val acc: 0.95, duration: 13.1s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.96, val loss: 0.00, val acc: 0.96, duration: 13.0s\n",
      "Training gap 10 with oversampling regular.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.15, val loss: 0.05, val acc: 0.26, duration: 8.2s\n",
      "Epoch   5/ 20, train loss: 0.02, train acc: 0.71, val loss: 0.02, val acc: 0.69, duration: 8.3s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.82, val loss: 0.01, val acc: 0.84, duration: 8.4s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.88, val loss: 0.01, val acc: 0.89, duration: 8.5s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.92, duration: 8.6s\n",
      "Training gap 30 with oversampling regular.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.09, val loss: 0.06, val acc: 0.14, duration: 5.1s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.53, val loss: 0.03, val acc: 0.54, duration: 4.7s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.68, val loss: 0.02, val acc: 0.70, duration: 4.8s\n",
      "Epoch  15/ 20, train loss: 0.02, train acc: 0.75, val loss: 0.02, val acc: 0.73, duration: 4.7s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.79, val loss: 0.01, val acc: 0.81, duration: 5.3s\n",
      "Training gap 100 with oversampling regular.\n",
      "Epoch   1/ 20, train loss: 0.07, train acc: 0.06, val loss: 0.06, val acc: 0.14, duration: 2.8s\n",
      "Epoch   5/ 20, train loss: 0.04, train acc: 0.46, val loss: 0.04, val acc: 0.47, duration: 2.3s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.70, val loss: 0.02, val acc: 0.64, duration: 2.3s\n",
      "Epoch  15/ 20, train loss: 0.02, train acc: 0.79, val loss: 0.03, val acc: 0.59, duration: 2.6s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.83, val loss: 0.02, val acc: 0.66, duration: 2.5s\n",
      "Training gap 1 with oversampling sqrt.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.26, val loss: 0.03, val acc: 0.43, duration: 17.2s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.82, val loss: 0.01, val acc: 0.83, duration: 17.4s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.93, duration: 17.5s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.94, val loss: 0.00, val acc: 0.96, duration: 17.4s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.96, val loss: 0.00, val acc: 0.97, duration: 20.7s\n",
      "Training gap 3 with oversampling sqrt.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.20, val loss: 0.04, val acc: 0.33, duration: 13.2s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.83, val loss: 0.01, val acc: 0.71, duration: 14.8s\n",
      "Epoch  10/ 20, train loss: 0.00, train acc: 0.92, val loss: 0.00, val acc: 0.92, duration: 14.0s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.95, val loss: 0.00, val acc: 0.96, duration: 13.7s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.96, val loss: 0.00, val acc: 0.97, duration: 13.6s\n",
      "Training gap 10 with oversampling sqrt.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.14, val loss: 0.05, val acc: 0.18, duration: 8.8s\n",
      "Epoch   5/ 20, train loss: 0.02, train acc: 0.72, val loss: 0.02, val acc: 0.55, duration: 8.7s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.83, val loss: 0.01, val acc: 0.76, duration: 8.6s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.88, val loss: 0.01, val acc: 0.88, duration: 8.4s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.92, duration: 8.7s\n",
      "Training gap 30 with oversampling sqrt.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.10, val loss: 0.06, val acc: 0.11, duration: 4.9s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.52, val loss: 0.03, val acc: 0.45, duration: 5.1s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.68, val loss: 0.02, val acc: 0.64, duration: 4.9s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.75, val loss: 0.02, val acc: 0.72, duration: 5.1s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.79, val loss: 0.01, val acc: 0.79, duration: 4.9s\n",
      "Training gap 100 with oversampling sqrt.\n",
      "Epoch   1/ 20, train loss: 0.06, train acc: 0.10, val loss: 0.06, val acc: 0.04, duration: 2.4s\n",
      "Epoch   5/ 20, train loss: 0.04, train acc: 0.41, val loss: 0.04, val acc: 0.29, duration: 2.5s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.68, val loss: 0.03, val acc: 0.58, duration: 2.4s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.78, val loss: 0.02, val acc: 0.65, duration: 2.6s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.84, val loss: 0.01, val acc: 0.80, duration: 2.5s\n",
      "Training gap 1 with oversampling beta.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.24, val loss: 0.04, val acc: 0.42, duration: 17.6s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.83, val loss: 0.01, val acc: 0.85, duration: 17.4s\n",
      "Epoch  10/ 20, train loss: 0.00, train acc: 0.92, val loss: 0.00, val acc: 0.93, duration: 18.5s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.95, val loss: 0.00, val acc: 0.96, duration: 17.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.97, val loss: 0.00, val acc: 0.98, duration: 17.3s\n",
      "Training gap 3 with oversampling beta.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.20, val loss: 0.04, val acc: 0.36, duration: 13.3s\n",
      "Epoch   5/ 20, train loss: 0.01, train acc: 0.81, val loss: 0.01, val acc: 0.83, duration: 13.4s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.91, val loss: 0.00, val acc: 0.92, duration: 14.1s\n",
      "Epoch  15/ 20, train loss: 0.00, train acc: 0.94, val loss: 0.00, val acc: 0.95, duration: 13.4s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.96, val loss: 0.00, val acc: 0.97, duration: 13.3s\n",
      "Training gap 10 with oversampling beta.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.15, val loss: 0.05, val acc: 0.11, duration: 8.5s\n",
      "Epoch   5/ 20, train loss: 0.02, train acc: 0.72, val loss: 0.02, val acc: 0.58, duration: 8.7s\n",
      "Epoch  10/ 20, train loss: 0.01, train acc: 0.84, val loss: 0.01, val acc: 0.83, duration: 8.6s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.89, val loss: 0.01, val acc: 0.86, duration: 8.8s\n",
      "Epoch  20/ 20, train loss: 0.00, train acc: 0.92, val loss: 0.00, val acc: 0.92, duration: 8.9s\n",
      "Training gap 30 with oversampling beta.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.18, val loss: 0.06, val acc: 0.08, duration: 4.9s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.56, val loss: 0.03, val acc: 0.38, duration: 5.0s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.71, val loss: 0.02, val acc: 0.55, duration: 4.8s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.78, val loss: 0.02, val acc: 0.70, duration: 5.1s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.82, val loss: 0.01, val acc: 0.73, duration: 5.1s\n",
      "Training gap 100 with oversampling beta.\n",
      "Epoch   1/ 20, train loss: 0.05, train acc: 0.29, val loss: 0.08, val acc: 0.02, duration: 3.0s\n",
      "Epoch   5/ 20, train loss: 0.03, train acc: 0.54, val loss: 0.06, val acc: 0.16, duration: 2.6s\n",
      "Epoch  10/ 20, train loss: 0.02, train acc: 0.70, val loss: 0.04, val acc: 0.27, duration: 2.4s\n",
      "Epoch  15/ 20, train loss: 0.01, train acc: 0.80, val loss: 0.03, val acc: 0.53, duration: 2.4s\n",
      "Epoch  20/ 20, train loss: 0.01, train acc: 0.84, val loss: 0.02, val acc: 0.61, duration: 2.8s\n"
     ]
    }
   ],
   "source": [
    "# Training and validating the various oversampling models\n",
    "for oversampling in [\"none\", \"regular\", \"sqrt\", \"beta\"]:\n",
    "    for class_gap in [1, 3, 10, 30, 100]:\n",
    "        print(f\"Training gap {class_gap} with oversampling {oversampling}.\")\n",
    "        train_evaluate(\n",
    "            label = \"transformer_oversampling-\" + oversampling + \"_\" + str(class_gap),\n",
    "            class_gap = class_gap,\n",
    "            oversampling = oversampling,\n",
    "            model_parameters = {\n",
    "                \"type\": \"transformer\",\n",
    "                \"vocab_len\": 24, \n",
    "                \"embed_size\": 64,\n",
    "                \"hidden_dim\": 128, \n",
    "                \"feed_forward_dim\": 128,\n",
    "                \"dropout_pos\": 0.2, \n",
    "                \"dropout_transformer\": 0.2,\n",
    "                \"dropout_class\": 0.2, \n",
    "                \"num_heads\": 4, \n",
    "                \"num_layers\": 2,\n",
    "            },\n",
    "            epochs=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1256bf0",
   "metadata": {},
   "source": [
    "# Oversampling Experiment Testing\n",
    "\n",
    "This experiment is explained in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd60262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"none\": \"oversampling-none\",\n",
    "    \"regular\": \"oversampling-regular\",\n",
    "    \"sqrt\": \"oversampling-sqrt\",\n",
    "    \"beta\": \"oversampling-effective\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "75ac68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_testing(models):\n",
    "    ''' returns the test loss and accuracy for the model dictionary input '''\n",
    "    test_loss = dict()\n",
    "    test_acc = dict()\n",
    "    for name, title in models.items():\n",
    "        class_gap_loss = dict()\n",
    "        class_gap_acc = dict()\n",
    "        for class_gap in [1, 3, 10, 30, 100]:\n",
    "            label = \"transformer_\" + title + \"_\" + str(class_gap)\n",
    "            loss, acc = evaluate(label, oversampling=\"regular\", class_gap=class_gap)\n",
    "            class_gap_loss[class_gap] = loss\n",
    "            class_gap_acc[class_gap] = acc\n",
    "        test_loss[name] = class_gap_loss\n",
    "        test_acc[name] = class_gap_acc\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb55e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = oversampling_testing(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ffb78a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling method:     none with class gap     1 has a loss of   0.00099\n",
      "Oversampling method:     none with class gap     3 has a loss of   0.00172\n",
      "Oversampling method:     none with class gap    10 has a loss of   0.00425\n",
      "Oversampling method:     none with class gap    30 has a loss of   0.01478\n",
      "Oversampling method:     none with class gap   100 has a loss of   0.02576\n",
      "\n",
      "Oversampling method:  regular with class gap     1 has a loss of   0.00129\n",
      "Oversampling method:  regular with class gap     3 has a loss of   0.00186\n",
      "Oversampling method:  regular with class gap    10 has a loss of   0.00495\n",
      "Oversampling method:  regular with class gap    30 has a loss of   0.01225\n",
      "Oversampling method:  regular with class gap   100 has a loss of   0.01958\n",
      "\n",
      "Oversampling method:     sqrt with class gap     1 has a loss of   0.00143\n",
      "Oversampling method:     sqrt with class gap     3 has a loss of   0.00135\n",
      "Oversampling method:     sqrt with class gap    10 has a loss of   0.00437\n",
      "Oversampling method:     sqrt with class gap    30 has a loss of   0.01261\n",
      "Oversampling method:     sqrt with class gap   100 has a loss of   0.01461\n",
      "\n",
      "Oversampling method:     beta with class gap     1 has a loss of   0.00144\n",
      "Oversampling method:     beta with class gap     3 has a loss of   0.00150\n",
      "Oversampling method:     beta with class gap    10 has a loss of   0.00480\n",
      "Oversampling method:     beta with class gap    30 has a loss of   0.01726\n",
      "Oversampling method:     beta with class gap   100 has a loss of   0.02138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for oversampling, tests in test_loss.items():\n",
    "    for class_gap, result in tests.items():\n",
    "        print(\"Oversampling method: %8s with class gap %5s has a loss of %9.5f\" % \\\n",
    "             (oversampling, class_gap, result))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1798cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling method:     none with class gap     1 has an accuracy of    98.19\n",
      "Oversampling method:     none with class gap     3 has an accuracy of    96.64\n",
      "Oversampling method:     none with class gap    10 has an accuracy of    91.91\n",
      "Oversampling method:     none with class gap    30 has an accuracy of    71.95\n",
      "Oversampling method:     none with class gap   100 has an accuracy of    54.50\n",
      "\n",
      "Oversampling method:  regular with class gap     1 has an accuracy of    97.50\n",
      "Oversampling method:  regular with class gap     3 has an accuracy of    96.42\n",
      "Oversampling method:  regular with class gap    10 has an accuracy of    91.51\n",
      "Oversampling method:  regular with class gap    30 has an accuracy of    78.82\n",
      "Oversampling method:  regular with class gap   100 has an accuracy of    64.00\n",
      "\n",
      "Oversampling method:     sqrt with class gap     1 has an accuracy of    97.29\n",
      "Oversampling method:     sqrt with class gap     3 has an accuracy of    97.56\n",
      "Oversampling method:     sqrt with class gap    10 has an accuracy of    92.23\n",
      "Oversampling method:     sqrt with class gap    30 has an accuracy of    77.66\n",
      "Oversampling method:     sqrt with class gap   100 has an accuracy of    75.92\n",
      "\n",
      "Oversampling method:     beta with class gap     1 has an accuracy of    97.50\n",
      "Oversampling method:     beta with class gap     3 has an accuracy of    97.21\n",
      "Oversampling method:     beta with class gap    10 has an accuracy of    91.22\n",
      "Oversampling method:     beta with class gap    30 has an accuracy of    65.93\n",
      "Oversampling method:     beta with class gap   100 has an accuracy of    61.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for oversampling, tests in test_acc.items():\n",
    "    for class_gap, result in tests.items():\n",
    "        result = result * 100\n",
    "        print(\"Oversampling method: %8s with class gap %5s has an accuracy of %8.2f\" % \\\n",
    "             (oversampling, class_gap, result))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d864cb",
   "metadata": {},
   "source": [
    "## Testing Best Model\n",
    "\n",
    "We have found that when it comes to dealing with an imbalanced distribution (we will be testing on the Iter100 dataset), the transformer model with the square root oversampling method performed best. Let's plot a confusion matrix to get a better look at how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a2482c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_f1(\n",
    "    ''' Evaluates the models against the test set returning the f1 score '''\n",
    "    label: str,\n",
    "    oversampling: str = \"none\",\n",
    "    num_classes: int = 100,\n",
    "    class_gap: int = 100,\n",
    "    max_seq_len: int = 128,\n",
    "    vocab_len = 24,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    amino_acids = [\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\n",
    "                   \"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"Y\",\"X\"]\n",
    "    mapping = {aa:i + 1 for i, aa in enumerate(amino_acids)}\n",
    "    mapping.update({'X': 21, 'U': 21, 'B': 21, 'O': 21, 'Z': 21})\n",
    "    \n",
    "    # Creating the datasets\n",
    "    test_dataset = PfamDataset(\"test\", num_classes, class_gap, \n",
    "                               mapping, max_seq_len, oversampling)\n",
    "    \n",
    "    # Weighted Random Sampler\n",
    "    sampler = WeightedRandomSampler(test_dataset.weights, test_dataset.len)\n",
    "    \n",
    "    # Creating the dataloaders\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=batch_size, \n",
    "                                                  num_workers=0, sampler=sampler)\n",
    "    \n",
    "    # Model\n",
    "    model = torch.load(\"model/\" + label + \".pt\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    num_val_correct = 0\n",
    "    num_val_examples = 0\n",
    "    \n",
    "    all_y = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_function(y_pred, y)\n",
    "            \n",
    "            all_y.append(y)\n",
    "            all_preds.append(y_pred)\n",
    "\n",
    "            val_loss += loss.data.item()\n",
    "            num_val_correct += (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().item()\n",
    "            num_val_examples += X.shape[0]\n",
    "\n",
    "        val_acc = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(test_dataloader.dataset)\n",
    "\n",
    "    # Calculate f1 score\n",
    "    all_preds = torch.cat(all_preds, dim=0).cpu().argmax(dim=1)\n",
    "    all_y = torch.cat(all_y, dim=0).cpu().argmax(dim=1)\n",
    "    \n",
    "    return f1_score(all_y, all_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7385ea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6719625422412902\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluation_f1(\"transformer_oversampling-sqrt_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392824aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
